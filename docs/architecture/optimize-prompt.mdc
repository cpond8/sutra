---
alwaysApply: false
---
# Prompt Preprocessing Protocol for AI Agents

## Embedded Rubric (Canonical, Verbatim)w

<!-- BEGIN: RUBRIC -->
**Summary:** This rubric empowers AI systems to self-optimize their prompts by breaking down prompt quality into concrete, fixable elements. Each dimension targets a critical aspect of prompt design – from clarity and completeness to format, reasoning, and consistency – that directly impacts an LLM's performance. By scoring and refining each area (e.g. ensuring unambiguous instructions, explicit output formats, positive constraints, and resistance to conversational drift), an AI can iteratively hone its guidance. This leads to more reliable, accurate, and on-target model outputs, as the prompt leaves little room for misinterpretation or error. Every dimension addresses known failure modes (like vague wording or forgetting instructions), so together they ensure high-quality operation and aligned, consistent responses from the LLM.

| **Dimension**                                 | **Description** (What it measures, with AI prompt example)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | **Scoring Levels (0–3)** <br>*0 = poor, 3 = excellent*                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | **Observable Signals** (for AI to detect in the prompt)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | **Automated Remediation** (suggested improvement if <3)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | **Source Reference**                                                                                                                                                                                                                                      |
| --------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Clarity & Specificity**                     | How clear, specific, and unambiguous the prompt's instructions are. Measures if the prompt tells exactly what the AI should do, without vague language or open-ended ambiguity. *Example:* Instead of "Write about OpenAI," a specific prompt would say "Write a **3-sentence** inspiring poem about OpenAI's **2023 DALL-E launch**, in the style of Maya Angelou."                                                                                                                                                                                                                    | **0:** Extremely vague or ambiguous request (e.g. "Tell me something interesting"); key details (context, style, length) are missing. <br> **1:** Some context or instruction given but important specifics are left open (e.g. "Write a short article about X" – unclear how short or which aspects). <br> **2:** Prompt provides clear primary instructions and some detail, but may omit one or two parameters (e.g. specifies topic and format but not tone or length). <br> **3:** Prompt is fully explicit about all important details – it pinpoints the exact task, scope, style, length, and any required focus, leaving virtually no room for misinterpretation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | - Presence of **ambiguous terms** ("some," "few," "short," "about," etc.) indicates lower clarity. <br>- Check if all **WH-questions** (who, what, when, how, format, etc.) implicit in the task are answered by the prompt. If any key detail (length, style, domain, target audience) is undefined, specificity is lacking. <br>- Look for multiple **specific descriptors** (e.g. exact number of items, target style or role, specific context). Their absence or vagueness (e.g. "a brief summary" without defining brief) signals a clarity gap.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | **Add or refine details** to eliminate ambiguity. For example, replace vague words with concrete specifics: instead of "a few sentences," say "**2-3 sentences**"; instead of "short article," specify "an article of **500 words**." Define any ambiguous requests (if "interesting" or "significant," state **in what way** or **to whom**). Explicitly answer any missing details – e.g. timeframe, perspective, or depth – so the prompt says *exactly* what is needed.                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Clarity improves model results when **prompts are detailed** about the task and desired output.                                                                                                                                                           |
| **Instruction Placement & Context Isolation** | Whether the prompt's directives are positioned and formatted to grab the model's attention, separate from any data or user input. High-scoring prompts put instructions at the very **beginning** and use delimiters or role designations to avoid confusion. *Example:* A prompt providing text to summarize will start with "Summarize the following text..." and use a clear separator (like `"""` or `###`) before the text.                                                                                                                                                        | **0:** Instructions are poorly placed or mixed in with content (e.g. instructions come after a long input, or not delineated at all), risking that the model misses or confuses them. <br> **1:** Instructions are present at or near the top but not clearly separated from the content (no delimiters or role tags), or minor contextual info precedes the main instruction. <br> **2:** Instructions are at the top and somewhat separated (e.g. a newline or simple phrase) but could be more explicit (no clear markers or section titles). <br> **3:** Instructions/header precede all other content, **clearly marked off** from any input data or examples (using separators like `###` or a System message), so the model knows exactly what the task is before seeing additional context.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | - Verify the **order**: Does the prompt begin with the task instructions or question? If not (e.g. starts with a user message or raw data), that's a red flag. <br>- Check for a **separator or formatting** (such as a line of `---`, `### Instruction:`, or quotes) between instructions and any provided text. Lack of a separator when context is included means instructions might blend into data. <br>- If using a chat format, see that the **system or role message** contains the instructions. If instructions are only in a user message after content, the model may have already "read" content without guidance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | **Reorder and format the prompt for clarity**: Put all directives at the very top. For example, prepend a clear instruction section like `### Instructions:` at the start. If the prompt includes input text or examples, insert a delimiter (e.g. triple quotes or a markdown separator) between the instruction and the input. Ensure the first thing the model "sees" is a well-marked instruction block stating the task. In multi-turn settings, consistently place or reference the main instructions in the system message or at each turn as needed.                                                                                                                                                                                                                                                                                                                                                                | Clear initial instructions, separated from context, help the model follow the request accurately.                                                                                                                                                         |
| **Output Format & Structure**                 | How well the prompt defines the desired output structure or format. A high score means the prompt explicitly tells the model *how* to present the answer (layout, format, or style), often by giving an example or template. *Example:* "Provide the answer as JSON with keys `foo` and `bar`, and no extra text," or "Answer in **three bullet points**."                                                                                                                                                                                                                              | **0:** No guidance on output format or structure – the model is left to decide how to present the answer (free-form). <br> **1:** Some format hints given (e.g. "list the points" or "in a report") but not specific enough or missing details (could lead to variable outputs). <br> **2:** Format is specified in general (e.g. "answer in bullet points" or "as JSON with these fields"), but might not cover edge cases or length limits (model could still stray with extra commentary or slight format deviations). <br> **3:** The prompt **fully specifies the output format** with no ambiguity – it might provide a template or example of the exact format (JSON keys, table columns, bullet count, etc.), and explicitly instructs the model to follow it *and nothing else*. This leaves no room for structural drift in any context.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | - Look for **format keywords** or examples: e.g. "list," "table," "JSON," "bullet points," or a shown layout ("Name: ..., Age: ..."). If none are present, format guidance is absent. <br>- If format instructions exist, check for **completeness**: do they specify all required fields/sections? (e.g. "include `X, Y, Z` in the output"). Vague format cues like "in a brief form" without specifics indicate partial credit. <br>- Notice if the prompt explicitly says **"Do not include"** any format deviations (like extraneous commentary). The presence of a clear directive like "Return **only** a JSON object with fields X, Y…" is a strong signal of full format control.                                                                                                                                                                                                                                                                                                                                                                                                                                          | **Specify the output blueprint.** Add a concrete format instruction or example if missing. For instance, if a structured output is needed, say: *"Respond with exactly three bullet points in Markdown."* If JSON or code is expected, explicitly provide a template or keys (e.g. *"Format the answer as: `{"field1": "...", "field2": "..."}` and nothing else."*). Include any required headings, bullet counts, or table columns in the prompt. When appropriate, show a **mini example** of the desired output format following the instruction (the model will mimic it). If the model tended to add extra prose, add a line like *"No additional explanation or text outside the required format."* to lock the structure.                                                                                                                                                                                           | Well-specified formats make outputs **consistent, parseable, and ready** for use, whereas unconstrained prompts can lead to rambling or unpredictable structure.                                                                                          |
| **Length & Detail Constraints**               | Evaluates if the prompt clearly defines the expected length or level of detail of the response. Good prompts remove guesswork by stating how long or detailed the answer should be. *Example:* "Provide a **3-5 sentence** summary" instead of "Provide a short summary."                                                                                                                                                                                                                                                                                                               | **0:** No length or detail guidance at all, when it's clearly needed (the model could respond with one sentence or a page – it's not specified). <br> **1:** Implicit or vague length cues (e.g. "briefly describe…" or "in a few words") that are open to interpretation. <br> **2:** Some length/detail guidance is given (like "a short paragraph" or "one paragraph answer"), but not precisely quantified; or only an upper bound (no more than 500 words – which prevents overlong output but doesn't ensure minimum depth). <br> **3:** Prompt explicitly quantifies or firmly bounds the response length/detail. This includes giving a specific range or exact length (sentences, words, paragraphs) or level of detail required. There is no uncertainty about how extensive the answer should be.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | - Spot any **numerical limits**: e.g. "2 sentences," "less than 100 words," "at most 3 bullet points." The presence of numbers or explicit limits indicates strong length control. <br>- If only qualitative terms like "briefly," "succinctly," "in short," are present without specifics, that's a weak signal (likely score 1). <br>- Check if the prompt uses comparative or unclear phrases ("not too much detail," "fairly short"). Those suggest the length instruction is not concrete. <br>- Absence of any length mention means the model must decide length itself (score 0 if length is important for the task).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | **Quantify or constrain the expected length.** If the prompt says "brief" or "few," replace that with an actual target. E.g., *"Limit the answer to 3 sentences."* or *"Respond in one paragraph (approximately 4-5 sentences)."* If the prompt gave no clue on length, add one based on what's needed (for instance, *"Provide 5 bullet points"* or *"In 100–150 words, explain…"*). Being explicit prevents the model from rambling or being too terse. Also specify detail level if needed (e.g. *"Include only the most essential facts"* if summary should be high-level).                                                                                                                                                                                                                                                                                                                                             | Models follow instructions more reliably when the prompt defines **how long** the response should be; explicit word/sentence limits turn imprecise "brief" requests into consistent outputs.                                                              |
| **Contextual Completeness & Relevance**       | Checks if the prompt provides all necessary context for the task and omits irrelevant or distracting information. A top score means the prompt gives the model exactly the info it needs (or references to it) and nothing extraneous. *Example:* If asking the AI to continue a story, the prompt includes the story so far. If summarizing text, the text is provided (or a link if the model has retrieval tools). Unneeded details or off-topic instructions are absent.                                                                                                            | **0:** Key context or data required to fulfill the request is missing (the model has to assume facts), or the prompt is cluttered with irrelevant info that confuses the task. <br> **1:** Some context is given but incomplete (e.g. partial data, or missing definitions) – the model might have to guess the rest. Alternatively, some unnecessary details are present that could lead the model astray. <br> **2:** Necessary context is mostly provided and prompt stays on-topic, but there might be minor omissions (a small piece of data or clarification not included) or minor extra text that isn't strictly needed. <br> **3:** The prompt is fully self-contained with relevant information: it includes all data, examples, or background needed for the task, and *only* pertinent details. Nothing important is left out, and nothing irrelevant is included to distract the model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | - If the task refers to a specific text, check that the text (or a summary of it) is included in the prompt. Missing input data or undefined references (e.g. "the article above" with no article) indicate low completeness. <br>- Look for **irrelevant sentences** or tangents not related to the instruction. Extra chit-chat or unrelated info in the prompt suggests lower relevance. <br>- Check for placeholders that haven't been replaced (like `{text}` or "XYZ") which mean required content is absent. <br>- Ensure any necessary **definitions or context** the model might not know are provided. If the prompt expects the model to use certain info (e.g. a schema, a user profile, earlier conversation), that info should appear or be referenced. If it's missing, that's a problem.                                                                                                                                                                                                                                                                                                                           | **Add any missing context** the model needs, and trim out anything extraneous. For missing info: if the prompt asks for analysis of a text or situation, include a summary or the actual text. If a question needs specific data (e.g. statistics, names, context), provide those upfront. For irrelevant content: remove or move off-topic details that don't contribute to the task. Ensure the prompt is **focused**: every sentence should support what you're asking the model to do. If the user's request is multi-part, ensure each part has the info needed to answer. In short, supply **only relevant background** and make sure no crucial data is left out.                                                                                                                                                                                                                                                    | Prompts work best when they contain just the necessary details – enough for the model to understand the task, but **no clutter** to confuse it.                                                                                                           |
| **Example Usage (Few-Shot)**                  | Assesses whether the prompt includes examples or demonstrations when appropriate to guide the model. Good prompts use **few-shot examples** or sample inputs/outputs to show the model exactly what is expected, especially for complex or format-specific tasks. *Example:* To teach a format, the prompt might say "Example Input: X -> Example Output: Y" before asking the model to produce an output for a new input.                                                                                                                                                              | **0:** No examples provided, even when the task is complex, ambiguous, or would clearly benefit from demonstration (the model has no guidance by example). <br> **1:** An example is attempted, but it's either incorrect, irrelevant, or too minimal to be helpful (e.g. only one very simple example for a complicated task, or an example that doesn't match the task scenario). <br> **2:** One or more examples are given that do illustrate the task, which significantly helps clarity, but edge cases or variations might not be covered (the examples cover common cases, though). Alternatively, examples are provided but not annotated or separated clearly. <br> **3:** The prompt includes a **set of well-chosen examples** (few-shot prompts) covering the task format or steps. They are clearly separated/marked (so the model knows they are examples), and they demonstrate exactly the pattern the model should follow. The model is then asked to do a similar task for a new input, making it very likely to imitate the examples.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | - Check for the presence of **example inputs and outputs** in the prompt (often delineated by numbering or labels like "Example 1", or by a format like Q: ... A: ...). If none, and the task isn't trivial, that's a sign of no examples. <br>- If examples are present, verify they actually match the task: e.g. if the task is translating English to Spanish, an example should show an English sentence and its Spanish translation. Irrelevant or wrong examples don't count. <br>- See if examples are **clearly distinguished** from the final query (using separators or descriptors like "Text 1: ... \n Answer 1: ..."). If the model might confuse example vs actual prompt, the examples aren't well integrated. <br>- More examples are generally needed for more complex tasks. A single example for something nuanced might score 1-2, whereas multiple illustrating different facets suggests a 3.                                                                                                                                                                                                               | **Add or improve examples** to guide the model. If the model is struggling or the task format is unusual, prepend a few-shot demonstration. For instance: *"Example 1: [Input] -> [Desired Output]"*, *"Example 2: [Input] -> [Desired Output]"*, then *"Now for the following input, provide the output:"*. Ensure the examples are **representative** of the task and formatted exactly as you want the model to respond. If an example is unclear, clarify it or add an explanation in the prompt. If only one example was given and it wasn't sufficient, add another that covers a different scenario or edge case. By showing the model what a correct response looks like, you steer it to produce similar outputs.                                                                                                                                                                                              | Few-shot prompting (giving examples) is a powerful way to **convey format and intent** – models respond better when shown what to do, not just told.                                                                                                      |
| **Step-by-Step Reasoning**                    | Whether the prompt encourages the model to reason through complex tasks in steps (Chain-of-Thought) rather than jumping to the answer. Important for math, logic, or multi-step problems. *Example:* Instead of "What is the result of XYZ?" a step-by-step prompt says: "**Let's work this out**: first do X, then do Y, then conclude Z."                                                                                                                                                                                                                                            | **0:** For tasks requiring reasoning, the prompt does not encourage any intermediate thinking (e.g. it just asks for an answer directly on a complex puzzle). The model is left to figure it out in one go. <br> **1:** The prompt hints at reasoning ("explain your answer" or "show steps") but does not structure it, or only requests reasoning after giving an answer. The guidance is minimal, so the model might skip some logic. <br> **2:** The prompt either explicitly says to reason (e.g. "think step by step") or breaks the query into sub-questions, but perhaps not as clearly or at the optimal point. The model is likely to show its reasoning, though the prompt could structure it more (e.g. doesn't use numbered steps when it could). <br> **3:** The prompt clearly instructs a step-by-step approach or otherwise scaffolds the reasoning process. It might use phrases like "First, … Then, … Finally, …" or ask intermediate questions. The model is guided to **show its work** before final answer, leading to more accurate and transparent results.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | - Look for explicit phrases like *"step by step," "first… next… last…," "let's think this through,"* or an instruction to *"explain your reasoning"*. Their presence is a strong signal the prompt is encouraging reasoning. <br>- If the problem is complex (math word problem, code debugging, etc.), absence of such cues suggests a likely 0 or 1. <br>- Check if the prompt breaks the task into **subtasks** or questions. For example, a multi-part instruction or bullet list guiding the solution process. If the prompt just asks a complex question in one go, it's not scaffolding reasoning. <br>- If the final answer is requested *after* an explanation (e.g. "Provide your reasoning, then give the final answer."), that indicates a thorough chain-of-thought prompt.                                                                                                                                                                                                                                                                                                                                           | **Incorporate a reasoning prompt.** Modify the prompt to guide the model's thought process. For instance, add: *"Let's solve this step by step."* Then outline steps: *"First, analyze…, Next, consider…, Finally, conclude with…"*. You can explicitly ask for an explanation: *"Explain your reasoning before giving the answer."* For complex tasks, you might break the query into multiple questions the model should answer in sequence (and possibly combine). By prompting the model to articulate intermediate steps, you help it avoid mistakes. Make sure to signal when it should stop reasoning and present the final answer (e.g. *"Finally, [the answer]"*).                                                                                                                                                                                                                                                | Prompting the model to **show its reasoning** leads to more accurate and reliable outputs on complex problems, because LLMs can otherwise skip logical steps – a chain-of-thought prompt fixes that.                                                      |
| **Role / Persona & Tone Guidance**            | Evaluates if the prompt sets a specific role, persona, or audience context for the model to adopt, as well as any tone or style guidelines. Useful for ensuring consistency and appropriateness of the response style. *Example:* "You are a customer support agent. Answer in a calm, friendly tone, as if speaking to a non-technical user."                                                                                                                                                                                                                                          | **0:** No role or tone guidance is given when it would be beneficial (the model must infer style, which may default to a generic or inappropriate tone). <br> **1:** A hint of style or perspective is present but very limited (e.g. "explain in simple terms" without saying to whom, or just "act as an expert" without further detail). Might not consistently enforce a persona. <br> **2:** The prompt defines either a role or an audience/tone, but not both, or does so in moderate detail. For example, it might say "You are a lawyer" *or* "use a formal tone," which helps, but could be more specific (e.g. what kind of lawyer, or what level of formality). <br> **3:** The prompt clearly establishes a specific role or identity for the AI *and/or* a target audience and tone, in detail. It might use a system message or an opening line like "You are a ${role}...". It also describes the desired tone/style (friendly, technical, casual, etc.) or audience (e.g. "for a 5-year-old child"). This ensures the model's response stays in character consistently.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | - Check for phrases that assign a **role or identity** to the AI: e.g. "You are a travel guide," "Act as a medical expert," "Answer as if you are speaking to a beginner." If such context-setting is present, that's a good sign. <br>- Look for any **tone descriptors**: "in a humorous tone," "formally," "simplify as if explaining to a child," etc. Their presence indicates the prompt is guiding style. <br>- If the content is domain-specific, see if the prompt indicates that domain knowledge or jargon level (e.g. "using legal terminology" or "explain like I'm new to this field"). <br>- Absence of any role/audience mention when the task could benefit from it (such as specialized advice or maintaining character in a story) means score 0. A generic chat with no context might not need it, but any hint that a certain perspective is expected should be explicitly set.                                                                                                                                                                                                                               | **Impose a role or style as needed.** Add a sentence at the start like: *"You are a **{expert/persona}** ..."* describing the role (e.g. teacher, programmer, therapist) relevant to the task. If the output should target a certain audience, state that (e.g. *"Explain this to a **middle-school student,"* or *"Use polite and professional language as if writing to a client."*). Include tone adjectives if important (friendly, academic, concise, enthusiastic, etc.). For example: *"Respond in a cheerful, encouraging tone."* This calibration ensures the model's response voice matches the intended style.                                                                                                                                                                                                                                                                                                 | Assigning the model a clear role or audience focus upfront leads to more **consistent and context-appropriate responses** (e.g., Claude can shift perspective instantly when given a role like "lawyer" or "therapist").                                  |
| **Constraints & Fail-safes**                  | How the prompt handles "dos and don'ts" – especially avoiding solely negative instructions – and whether it provides alternative actions or fallbacks if the model can't fulfill something. A good prompt not only says what to avoid, but also tells the model what to do instead, and how to respond if it lacks information. *Example:* Rather than just "Do NOT include personal info," a better prompt adds "...**instead, explain why you cannot provide that**." Similarly, if asked to fetch data, a fail-safe might be: "If the data isn't available, reply 'Data not found.'" | **0:** The prompt either: (a) only lists negative rules ("don't do X, don't say Y") without any guidance on what to do, leaving the model with a void to fill (which it might fill incorrectly); **or** (b) has no mention of important constraints, allowing the model to do disallowed or unwanted things. No fallback behavior is specified for when the task can't be done. <br> **1:** Some constraints are given, but they're mostly negative and not coupled with positive instructions. The model is told not to do something, but not clearly instructed on the preferred action instead (e.g. "Do not use jargon," but not telling what style to use). Fail-safes ("what if…") are not addressed. <br> **2:** The prompt includes both "don'ts" and "dos" for at least major issues, guiding the model towards acceptable behavior (e.g. "Don't X, do Y instead"). It might still miss a less obvious constraint or fail-safe. Possibly only partial fallback instructions (e.g. instructs what to do if missing info in one case but not another). <br> **3:** The prompt clearly spells out constraints in a positive way and covers alternatives and edge cases. Every "avoid doing X" is paired with a clear "do this instead" directive. If there's a scenario where the model might not have an answer or shouldn't comply, the prompt explicitly provides a fallback response or error handling (e.g. instructs the model to say "I cannot disclose that" or "not found" in such cases). The model is never left guessing how to handle a restriction – it's told exactly how to comply gracefully. | - Scan for the word **"not"** or phrases like "do not," "avoid," "never." If they appear, check if the prompt also tells what to do instead. Just prohibitions alone (e.g. "Do not use first person." with no further guidance) are a sign of missing positive direction. <br>- See if the prompt addresses potential failure conditions. Phrases like **"if/when … cannot …, then …"** indicate a fail-safe. For example, *"If the user asks for banned content, explain you cannot comply."* Without any such phrases, the model might not know what to do when it can't follow an instruction or find info. <br>- Check if constraints are specific. General "don't be wrong" or "don't be rude" are hard to follow; clear ones like "do not reveal personal data" plus "respond with a generic advice instead" are better. Look for those specific paired directives. <br>- If the task has known edge cases (e.g. user might input something out of scope), does the prompt mention how to handle them? Lack of it could lower the score.                                                                                     | **Reframe negatives into guided positives, and add fallbacks.** Wherever the prompt says "Do not do X," append or replace with **what to do instead**. For instance: *"Do not reveal confidential info; **if asked, respond with a polite refusal explaining you can't share it**."* This way the model isn't stuck when a forbidden action is in play – it knows the correct alternative action. Additionally, include explicit **error-handling** instructions: e.g. *"If the necessary data isn't available, reply 'Sorry, I cannot find that information.'"* or *"If the user's request violates policy, respond with a brief apology and refusal."* By giving an "out" for unsolvable cases or disallowed content, you prevent the model from either stalling or guessing. Essentially, tell the model how to gracefully handle **no-win scenarios** and constraints.                                                  | Instead of only forbidding actions, prompting best practice is to **tell the model what to do** in those cases. Providing a fallback ("respond with 'not found' if the info isn't present) keeps the model's output logical and prevents made-up answers. |
| **Drift Resistance (Multi-turn Consistency)** | Measures the prompt's ability to keep the model aligned and on track over multiple turns or a long session. A robust prompt anticipates conversational drift or forgetfulness and includes mechanisms to reinforce context and rules. *Example:* In a chatbot scenario, a system prompt might remind at each turn: "You are a helpful assistant. Remember: \[key rules]." The prompt might also instruct the model to summarize prior context if needed, or use a structured format to carry state, so it doesn't go off track as the dialogue progresses.                              | **0:** No consideration for maintaining context – the prompt is likely single-turn with no instructions to handle continuity. In multi-turn use, the model often "forgets" prior instructions or veers off-topic easily. No reminders or state management present. <br> **1:** Minor attempts at persistence (maybe an initial role is given, or the user repeats instructions occasionally), but the model still easily drifts if the conversation is long. Important constraints or context aren't reinforced, just stated once. <br> **2:** The prompt/framework includes some strategy to combat drift: e.g. key instructions are in a system message or there's a suggestion to recap context. The model mostly stays on track, though after many turns it might still degrade. The prompt could reinforce more often or more systematically. <br> **3:** The prompt is explicitly designed for multi-turn stability. It **reinforces core context or rules at each turn or whenever needed** – possibly via an automated summary of prior content, a persistent system message, or repeating crucial guidelines in each prompt. It might use structured representations of state (like JSON for memory) or ask the model to self-check consistency. The result is the model rarely forgets instructions or important facts even in long conversations, and recovers gracefully from user tangents.                                                                                                                                                                                                             | - In a multi-turn setting, see if the conversation uses a **system or persistent instruction** that carries over (score higher if yes). If it's just user and assistant messages with no system role reminding context, that's weaker. <br>- Check for any instructions about **maintaining context or consistency** (e.g. "stay consistent with the above information" or "do not forget your role as..."). Such phrases indicate drift prevention. <br>- Look for techniques like **context summaries or state tracking** in the prompt. For example, the presence of a summary of previous facts at each turn, or a format where the model's output includes a recap section. If the prompt explicitly says something like "Here are the rules: \[rules]. (The assistant should remember these in all responses.)", that's a strong anti-drift signal. <br>- Also, if using a tool or code, check if the prompt asks the model to output in a way that the tool can feed the model's own previous thoughts back (like a planning loop). Lack of any such elements means the model could easily wander after a few interactions. | **Reinforce and remind**. To improve drift resistance, modify the prompt or system message to **repeat key instructions or context each turn**. For example: *"(Reminder: You are an AI tutor, always explain in simple terms and stay on topic.)"* — include this at the start of each user prompt or as a constant system note. If the conversation is long, instruct the model to summarize important prior facts and its plan before proceeding, or maintain a structured memory (like keeping track of facts in a list). You can also explicitly tell the model to **self-check consistency**: e.g. *"Before answering, recall the user's goal and the rules above."* Another tactic is to use a fixed format every turn that carries state, like a JSON state that gets updated, preventing forgetting. Essentially, don't rely on the model's hidden memory alone – **bake the context into the prompt repeatedly**. | Long dialogues can confuse LLMs; effective prompts mitigate this by **layering techniques to preserve context**, like summaries, role reminders, and structured memory, ensuring the model doesn't lose track of instructions or prior facts.             |

**AI Feedback Loop:** An agentic AI can utilize the above rubric by first generating a draft prompt and then scoring it on each dimension. For any criterion where the score is not a 3, the AI applies the suggested "Automated Remediation" directly to improve the prompt (e.g. adding a missing format example or clarifying ambiguous wording). It can then re-evaluate the revised prompt against the rubric, iterating in this manner until all dimensions reach a 3. This loop allows the AI to systematically refine its instructions, resulting in a high-quality, optimized prompt. Over time, the agent can continuously monitor conversations and adjust the prompt if drift or new issues arise, thereby maintaining prompt effectiveness in a dynamic setting.

<!-- END: RUBRIC -->

---

## Prompt Preprocessing Protocol

### Trigger Conditions

Activate preprocessing if the prompt meets any of the following:

- Length exceeds 3 sentences.
- Contains multi-step instructions, workflow setup, or detailed system prompt content.
- Is explicitly marked as complex or high-impact by user/system.

### Preprocessing Steps

1. **Evaluate Prompt:**
   - Systematically assess the prompt against each rubric dimension, assigning a clear numeric score (0-3) for each dimension.
2. **Automated Remediation:**
   - For every rubric dimension scoring below the maximum (3):
     - Apply the rubric's prescribed remediation directly to the prompt, rewriting it for optimal compliance.
3. **Iterative Refinement:**
   - Repeat the evaluation and remediation steps (up to 3 iterations) until:
     - All rubric dimensions score the maximum (3), or
     - No further improvements are achievable.
4. **Replace Original Prompt:**
   - Use the final, optimized version of the prompt for all subsequent processing.

### Exception Handling

- Do not preprocess trivial, casual, or single-question prompts.
- If the rubric is missing, incomplete, or unclear:
  - Halt execution immediately.
  - Issue an explicit error message requesting clarification or complete rubric provision.

### Transparency (Optional)

- If permitted by system policies, record the evaluation scores, specific remediation actions taken, and the iterative changes made for auditing or debugging purposes.

### Constraints and Requirements

- Do not summarize, modify, omit, or reinterpret these instructions.
- Avoid self-reflection, explanation of actions, or meta-commentary unless explicitly directed to provide such.
- Halt execution and request clarification if any ambiguity or conflicting instructions arise.
# Prompt Preprocessing Protocol for AI Agents

## Embedded Rubric (Canonical, Verbatim)

<!-- BEGIN: RUBRIC -->
**Summary:** This rubric empowers AI systems to self-optimize their prompts by breaking down prompt quality into concrete, fixable elements. Each dimension targets a critical aspect of prompt design – from clarity and completeness to format, reasoning, and consistency – that directly impacts an LLM's performance. By scoring and refining each area (e.g. ensuring unambiguous instructions, explicit output formats, positive constraints, and resistance to conversational drift), an AI can iteratively hone its guidance. This leads to more reliable, accurate, and on-target model outputs, as the prompt leaves little room for misinterpretation or error. Every dimension addresses known failure modes (like vague wording or forgetting instructions), so together they ensure high-quality operation and aligned, consistent responses from the LLM.

| **Dimension**                                 | **Description** (What it measures, with AI prompt example)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | **Scoring Levels (0–3)** <br>*0 = poor, 3 = excellent*                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | **Observable Signals** (for AI to detect in the prompt)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | **Automated Remediation** (suggested improvement if <3)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | **Source Reference**                                                                                                                                                                                                                                      |
| --------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Clarity & Specificity**                     | How clear, specific, and unambiguous the prompt's instructions are. Measures if the prompt tells exactly what the AI should do, without vague language or open-ended ambiguity. *Example:* Instead of "Write about OpenAI," a specific prompt would say "Write a **3-sentence** inspiring poem about OpenAI's **2023 DALL-E launch**, in the style of Maya Angelou."                                                                                                                                                                                                                    | **0:** Extremely vague or ambiguous request (e.g. "Tell me something interesting"); key details (context, style, length) are missing. <br> **1:** Some context or instruction given but important specifics are left open (e.g. "Write a short article about X" – unclear how short or which aspects). <br> **2:** Prompt provides clear primary instructions and some detail, but may omit one or two parameters (e.g. specifies topic and format but not tone or length). <br> **3:** Prompt is fully explicit about all important details – it pinpoints the exact task, scope, style, length, and any required focus, leaving virtually no room for misinterpretation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | - Presence of **ambiguous terms** ("some," "few," "short," "about," etc.) indicates lower clarity. <br>- Check if all **WH-questions** (who, what, when, how, format, etc.) implicit in the task are answered by the prompt. If any key detail (length, style, domain, target audience) is undefined, specificity is lacking. <br>- Look for multiple **specific descriptors** (e.g. exact number of items, target style or role, specific context). Their absence or vagueness (e.g. "a brief summary" without defining brief) signals a clarity gap.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | **Add or refine details** to eliminate ambiguity. For example, replace vague words with concrete specifics: instead of "a few sentences," say "**2-3 sentences**"; instead of "short article," specify "an article of **500 words**." Define any ambiguous requests (if "interesting" or "significant," state **in what way** or **to whom**). Explicitly answer any missing details – e.g. timeframe, perspective, or depth – so the prompt says *exactly* what is needed.                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Clarity improves model results when **prompts are detailed** about the task and desired output.                                                                                                                                                           |
| **Instruction Placement & Context Isolation** | Whether the prompt's directives are positioned and formatted to grab the model's attention, separate from any data or user input. High-scoring prompts put instructions at the very **beginning** and use delimiters or role designations to avoid confusion. *Example:* A prompt providing text to summarize will start with "Summarize the following text..." and use a clear separator (like `"""` or `###`) before the text.                                                                                                                                                        | **0:** Instructions are poorly placed or mixed in with content (e.g. instructions come after a long input, or not delineated at all), risking that the model misses or confuses them. <br> **1:** Instructions are present at or near the top but not clearly separated from the content (no delimiters or role tags), or minor contextual info precedes the main instruction. <br> **2:** Instructions are at the top and somewhat separated (e.g. a newline or simple phrase) but could be more explicit (no clear markers or section titles). <br> **3:** Instructions/header precede all other content, **clearly marked off** from any input data or examples (using separators like `###` or a System message), so the model knows exactly what the task is before seeing additional context.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | - Verify the **order**: Does the prompt begin with the task instructions or question? If not (e.g. starts with a user message or raw data), that's a red flag. <br>- Check for a **separator or formatting** (such as a line of `---`, `### Instruction:`, or quotes) between instructions and any provided text. Lack of a separator when context is included means instructions might blend into data. <br>- If using a chat format, see that the **system or role message** contains the instructions. If instructions are only in a user message after content, the model may have already "read" content without guidance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | **Reorder and format the prompt for clarity**: Put all directives at the very top. For example, prepend a clear instruction section like `### Instructions:` at the start. If the prompt includes input text or examples, insert a delimiter (e.g. triple quotes or a markdown separator) between the instruction and the input. Ensure the first thing the model "sees" is a well-marked instruction block stating the task. In multi-turn settings, consistently place or reference the main instructions in the system message or at each turn as needed.                                                                                                                                                                                                                                                                                                                                                                | Clear initial instructions, separated from context, help the model follow the request accurately.                                                                                                                                                         |
| **Output Format & Structure**                 | How well the prompt defines the desired output structure or format. A high score means the prompt explicitly tells the model *how* to present the answer (layout, format, or style), often by giving an example or template. *Example:* "Provide the answer as JSON with keys `foo` and `bar`, and no extra text," or "Answer in **three bullet points**."                                                                                                                                                                                                                              | **0:** No guidance on output format or structure – the model is left to decide how to present the answer (free-form). <br> **1:** Some format hints given (e.g. "list the points" or "in a report") but not specific enough or missing details (could lead to variable outputs). <br> **2:** Format is specified in general (e.g. "answer in bullet points" or "as JSON with these fields"), but might not cover edge cases or length limits (model could still stray with extra commentary or slight format deviations). <br> **3:** The prompt **fully specifies the output format** with no ambiguity – it might provide a template or example of the exact format (JSON keys, table columns, bullet count, etc.), and explicitly instructs the model to follow it *and nothing else*. This leaves no room for structural drift in any context.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | - Look for **format keywords** or examples: e.g. "list," "table," "JSON," "bullet points," or a shown layout ("Name: ..., Age: ..."). If none are present, format guidance is absent. <br>- If format instructions exist, check for **completeness**: do they specify all required fields/sections? (e.g. "include `X, Y, Z` in the output"). Vague format cues like "in a brief form" without specifics indicate partial credit. <br>- Notice if the prompt explicitly says **"Do not include"** any format deviations (like extraneous commentary). The presence of a clear directive like "Return **only** a JSON object with fields X, Y…" is a strong signal of full format control.                                                                                                                                                                                                                                                                                                                                                                                                                                          | **Specify the output blueprint.** Add a concrete format instruction or example if missing. For instance, if a structured output is needed, say: *"Respond with exactly three bullet points in Markdown."* If JSON or code is expected, explicitly provide a template or keys (e.g. *"Format the answer as: `{"field1": "...", "field2": "..."}` and nothing else."*). Include any required headings, bullet counts, or table columns in the prompt. When appropriate, show a **mini example** of the desired output format following the instruction (the model will mimic it). If the model tended to add extra prose, add a line like *"No additional explanation or text outside the required format."* to lock the structure.                                                                                                                                                                                           | Well-specified formats make outputs **consistent, parseable, and ready** for use, whereas unconstrained prompts can lead to rambling or unpredictable structure.                                                                                          |
| **Length & Detail Constraints**               | Evaluates if the prompt clearly defines the expected length or level of detail of the response. Good prompts remove guesswork by stating how long or detailed the answer should be. *Example:* "Provide a **3-5 sentence** summary" instead of "Provide a short summary."                                                                                                                                                                                                                                                                                                               | **0:** No length or detail guidance at all, when it's clearly needed (the model could respond with one sentence or a page – it's not specified). <br> **1:** Implicit or vague length cues (e.g. "briefly describe…" or "in a few words") that are open to interpretation. <br> **2:** Some length/detail guidance is given (like "a short paragraph" or "one paragraph answer"), but not precisely quantified; or only an upper bound (no more than 500 words – which prevents overlong output but doesn't ensure minimum depth). <br> **3:** Prompt explicitly quantifies or firmly bounds the response length/detail. This includes giving a specific range or exact length (sentences, words, paragraphs) or level of detail required. There is no uncertainty about how extensive the answer should be.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | - Spot any **numerical limits**: e.g. "2 sentences," "less than 100 words," "at most 3 bullet points." The presence of numbers or explicit limits indicates strong length control. <br>- If only qualitative terms like "briefly," "succinctly," "in short," are present without specifics, that's a weak signal (likely score 1). <br>- Check if the prompt uses comparative or unclear phrases ("not too much detail," "fairly short"). Those suggest the length instruction is not concrete. <br>- Absence of any length mention means the model must decide length itself (score 0 if length is important for the task).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | **Quantify or constrain the expected length.** If the prompt says "brief" or "few," replace that with an actual target. E.g., *"Limit the answer to 3 sentences."* or *"Respond in one paragraph (approximately 4-5 sentences)."* If the prompt gave no clue on length, add one based on what's needed (for instance, *"Provide 5 bullet points"* or *"In 100–150 words, explain…"*). Being explicit prevents the model from rambling or being too terse. Also specify detail level if needed (e.g. *"Include only the most essential facts"* if summary should be high-level).                                                                                                                                                                                                                                                                                                                                             | Models follow instructions more reliably when the prompt defines **how long** the response should be; explicit word/sentence limits turn imprecise "brief" requests into consistent outputs.                                                              |
| **Contextual Completeness & Relevance**       | Checks if the prompt provides all necessary context for the task and omits irrelevant or distracting information. A top score means the prompt gives the model exactly the info it needs (or references to it) and nothing extraneous. *Example:* If asking the AI to continue a story, the prompt includes the story so far. If summarizing text, the text is provided (or a link if the model has retrieval tools). Unneeded details or off-topic instructions are absent.                                                                                                            | **0:** Key context or data required to fulfill the request is missing (the model has to assume facts), or the prompt is cluttered with irrelevant info that confuses the task. <br> **1:** Some context is given but incomplete (e.g. partial data, or missing definitions) – the model might have to guess the rest. Alternatively, some unnecessary details are present that could lead the model astray. <br> **2:** Necessary context is mostly provided and prompt stays on-topic, but there might be minor omissions (a small piece of data or clarification not included) or minor extra text that isn't strictly needed. <br> **3:** The prompt is fully self-contained with relevant information: it includes all data, examples, or background needed for the task, and *only* pertinent details. Nothing important is left out, and nothing irrelevant is included to distract the model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | - If the task refers to a specific text, check that the text (or a summary of it) is included in the prompt. Missing input data or undefined references (e.g. "the article above" with no article) indicate low completeness. <br>- Look for **irrelevant sentences** or tangents not related to the instruction. Extra chit-chat or unrelated info in the prompt suggests lower relevance. <br>- Check for placeholders that haven't been replaced (like `{text}` or "XYZ") which mean required content is absent. <br>- Ensure any necessary **definitions or context** the model might not know are provided. If the prompt expects the model to use certain info (e.g. a schema, a user profile, earlier conversation), that info should appear or be referenced. If it's missing, that's a problem.                                                                                                                                                                                                                                                                                                                           | **Add any missing context** the model needs, and trim out anything extraneous. For missing info: if the prompt asks for analysis of a text or situation, include a summary or the actual text. If a question needs specific data (e.g. statistics, names, context), provide those upfront. For irrelevant content: remove or move off-topic details that don't contribute to the task. Ensure the prompt is **focused**: every sentence should support what you're asking the model to do. If the user's request is multi-part, ensure each part has the info needed to answer. In short, supply **only relevant background** and make sure no crucial data is left out.                                                                                                                                                                                                                                                    | Prompts work best when they contain just the necessary details – enough for the model to understand the task, but **no clutter** to confuse it.                                                                                                           |
| **Example Usage (Few-Shot)**                  | Assesses whether the prompt includes examples or demonstrations when appropriate to guide the model. Good prompts use **few-shot examples** or sample inputs/outputs to show the model exactly what is expected, especially for complex or format-specific tasks. *Example:* To teach a format, the prompt might say "Example Input: X -> Example Output: Y" before asking the model to produce an output for a new input.                                                                                                                                                              | **0:** No examples provided, even when the task is complex, ambiguous, or would clearly benefit from demonstration (the model has no guidance by example). <br> **1:** An example is attempted, but it's either incorrect, irrelevant, or too minimal to be helpful (e.g. only one very simple example for a complicated task, or an example that doesn't match the task scenario). <br> **2:** One or more examples are given that do illustrate the task, which significantly helps clarity, but edge cases or variations might not be covered (the examples cover common cases, though). Alternatively, examples are provided but not annotated or separated clearly. <br> **3:** The prompt includes a **set of well-chosen examples** (few-shot prompts) covering the task format or steps. They are clearly separated/marked (so the model knows they are examples), and they demonstrate exactly the pattern the model should follow. The model is then asked to do a similar task for a new input, making it very likely to imitate the examples.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | - Check for the presence of **example inputs and outputs** in the prompt (often delineated by numbering or labels like "Example 1", or by a format like Q: ... A: ...). If none, and the task isn't trivial, that's a sign of no examples. <br>- If examples are present, verify they actually match the task: e.g. if the task is translating English to Spanish, an example should show an English sentence and its Spanish translation. Irrelevant or wrong examples don't count. <br>- See if examples are **clearly distinguished** from the final query (using separators or descriptors like "Text 1: ... \n Answer 1: ..."). If the model might confuse example vs actual prompt, the examples aren't well integrated. <br>- More examples are generally needed for more complex tasks. A single example for something nuanced might score 1-2, whereas multiple illustrating different facets suggests a 3.                                                                                                                                                                                                               | **Add or improve examples** to guide the model. If the model is struggling or the task format is unusual, prepend a few-shot demonstration. For instance: *"Example 1: [Input] -> [Desired Output]"*, *"Example 2: [Input] -> [Desired Output]"*, then *"Now for the following input, provide the output:"*. Ensure the examples are **representative** of the task and formatted exactly as you want the model to respond. If an example is unclear, clarify it or add an explanation in the prompt. If only one example was given and it wasn't sufficient, add another that covers a different scenario or edge case. By showing the model what a correct response looks like, you steer it to produce similar outputs.                                                                                                                                                                                              | Few-shot prompting (giving examples) is a powerful way to **convey format and intent** – models respond better when shown what to do, not just told.                                                                                                      |
| **Step-by-Step Reasoning**                    | Whether the prompt encourages the model to reason through complex tasks in steps (Chain-of-Thought) rather than jumping to the answer. Important for math, logic, or multi-step problems. *Example:* Instead of "What is the result of XYZ?" a step-by-step prompt says: "**Let's work this out**: first do X, then do Y, then conclude Z."                                                                                                                                                                                                                                            | **0:** For tasks requiring reasoning, the prompt does not encourage any intermediate thinking (e.g. it just asks for an answer directly on a complex puzzle). The model is left to figure it out in one go. <br> **1:** The prompt hints at reasoning ("explain your answer" or "show steps") but does not structure it, or only requests reasoning after giving an answer. The guidance is minimal, so the model might skip some logic. <br> **2:** The prompt either explicitly says to reason (e.g. "think step by step") or breaks the query into sub-questions, but perhaps not as clearly or at the optimal point. The model is likely to show its reasoning, though the prompt could structure it more (e.g. doesn't use numbered steps when it could). <br> **3:** The prompt clearly instructs a step-by-step approach or otherwise scaffolds the reasoning process. It might use phrases like "First, … Then, … Finally, …" or ask intermediate questions. The model is guided to **show its work** before final answer, leading to more accurate and transparent results.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | - Look for explicit phrases like *"step by step," "first… next… last…," "let's think this through,"* or an instruction to *"explain your reasoning"*. Their presence is a strong signal the prompt is encouraging reasoning. <br>- If the problem is complex (math word problem, code debugging, etc.), absence of such cues suggests a likely 0 or 1. <br>- Check if the prompt breaks the task into **subtasks** or questions. For example, a multi-part instruction or bullet list guiding the solution process. If the prompt just asks a complex question in one go, it's not scaffolding reasoning. <br>- If the final answer is requested *after* an explanation (e.g. "Provide your reasoning, then give the final answer."), that indicates a thorough chain-of-thought prompt.                                                                                                                                                                                                                                                                                                                                           | **Incorporate a reasoning prompt.** Modify the prompt to guide the model's thought process. For instance, add: *"Let's solve this step by step."* Then outline steps: *"First, analyze…, Next, consider…, Finally, conclude with…"*. You can explicitly ask for an explanation: *"Explain your reasoning before giving the answer."* For complex tasks, you might break the query into multiple questions the model should answer in sequence (and possibly combine). By prompting the model to articulate intermediate steps, you help it avoid mistakes. Make sure to signal when it should stop reasoning and present the final answer (e.g. *"Finally, [the answer]"*).                                                                                                                                                                                                                                                | Prompting the model to **show its reasoning** leads to more accurate and reliable outputs on complex problems, because LLMs can otherwise skip logical steps – a chain-of-thought prompt fixes that.                                                      |
| **Role / Persona & Tone Guidance**            | Evaluates if the prompt sets a specific role, persona, or audience context for the model to adopt, as well as any tone or style guidelines. Useful for ensuring consistency and appropriateness of the response style. *Example:* "You are a customer support agent. Answer in a calm, friendly tone, as if speaking to a non-technical user."                                                                                                                                                                                                                                          | **0:** No role or tone guidance is given when it would be beneficial (the model must infer style, which may default to a generic or inappropriate tone). <br> **1:** A hint of style or perspective is present but very limited (e.g. "explain in simple terms" without saying to whom, or just "act as an expert" without further detail). Might not consistently enforce a persona. <br> **2:** The prompt defines either a role or an audience/tone, but not both, or does so in moderate detail. For example, it might say "You are a lawyer" *or* "use a formal tone," which helps, but could be more specific (e.g. what kind of lawyer, or what level of formality). <br> **3:** The prompt clearly establishes a specific role or identity for the AI *and/or* a target audience and tone, in detail. It might use a system message or an opening line like "You are a ${role}...". It also describes the desired tone/style (friendly, technical, casual, etc.) or audience (e.g. "for a 5-year-old child"). This ensures the model's response stays in character consistently.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | - Check for phrases that assign a **role or identity** to the AI: e.g. "You are a travel guide," "Act as a medical expert," "Answer as if you are speaking to a beginner." If such context-setting is present, that's a good sign. <br>- Look for any **tone descriptors**: "in a humorous tone," "formally," "simplify as if explaining to a child," etc. Their presence indicates the prompt is guiding style. <br>- If the content is domain-specific, see if the prompt indicates that domain knowledge or jargon level (e.g. "using legal terminology" or "explain like I'm new to this field"). <br>- Absence of any role/audience mention when the task could benefit from it (such as specialized advice or maintaining character in a story) means score 0. A generic chat with no context might not need it, but any hint that a certain perspective is expected should be explicitly set.                                                                                                                                                                                                                               | **Impose a role or style as needed.** Add a sentence at the start like: *"You are a **{expert/persona}** ..."* describing the role (e.g. teacher, programmer, therapist) relevant to the task. If the output should target a certain audience, state that (e.g. *"Explain this to a **middle-school student,"* or *"Use polite and professional language as if writing to a client."*). Include tone adjectives if important (friendly, academic, concise, enthusiastic, etc.). For example: *"Respond in a cheerful, encouraging tone."* This calibration ensures the model's response voice matches the intended style.                                                                                                                                                                                                                                                                                                 | Assigning the model a clear role or audience focus upfront leads to more **consistent and context-appropriate responses** (e.g., Claude can shift perspective instantly when given a role like "lawyer" or "therapist").                                  |
| **Constraints & Fail-safes**                  | How the prompt handles "dos and don'ts" – especially avoiding solely negative instructions – and whether it provides alternative actions or fallbacks if the model can't fulfill something. A good prompt not only says what to avoid, but also tells the model what to do instead, and how to respond if it lacks information. *Example:* Rather than just "Do NOT include personal info," a better prompt adds "...**instead, explain why you cannot provide that**." Similarly, if asked to fetch data, a fail-safe might be: "If the data isn't available, reply 'Data not found.'" | **0:** The prompt either: (a) only lists negative rules ("don't do X, don't say Y") without any guidance on what to do, leaving the model with a void to fill (which it might fill incorrectly); **or** (b) has no mention of important constraints, allowing the model to do disallowed or unwanted things. No fallback behavior is specified for when the task can't be done. <br> **1:** Some constraints are given, but they're mostly negative and not coupled with positive instructions. The model is told not to do something, but not clearly instructed on the preferred action instead (e.g. "Do not use jargon," but not telling what style to use). Fail-safes ("what if…") are not addressed. <br> **2:** The prompt includes both "don'ts" and "dos" for at least major issues, guiding the model towards acceptable behavior (e.g. "Don't X, do Y instead"). It might still miss a less obvious constraint or fail-safe. Possibly only partial fallback instructions (e.g. instructs what to do if missing info in one case but not another). <br> **3:** The prompt clearly spells out constraints in a positive way and covers alternatives and edge cases. Every "avoid doing X" is paired with a clear "do this instead" directive. If there's a scenario where the model might not have an answer or shouldn't comply, the prompt explicitly provides a fallback response or error handling (e.g. instructs the model to say "I cannot disclose that" or "not found" in such cases). The model is never left guessing how to handle a restriction – it's told exactly how to comply gracefully. | - Scan for the word **"not"** or phrases like "do not," "avoid," "never." If they appear, check if the prompt also tells what to do instead. Just prohibitions alone (e.g. "Do not use first person." with no further guidance) are a sign of missing positive direction. <br>- See if the prompt addresses potential failure conditions. Phrases like **"if/when … cannot …, then …"** indicate a fail-safe. For example, *"If the user asks for banned content, explain you cannot comply."* Without any such phrases, the model might not know what to do when it can't follow an instruction or find info. <br>- Check if constraints are specific. General "don't be wrong" or "don't be rude" are hard to follow; clear ones like "do not reveal personal data" plus "respond with a generic advice instead" are better. Look for those specific paired directives. <br>- If the task has known edge cases (e.g. user might input something out of scope), does the prompt mention how to handle them? Lack of it could lower the score.                                                                                     | **Reframe negatives into guided positives, and add fallbacks.** Wherever the prompt says "Do not do X," append or replace with **what to do instead**. For instance: *"Do not reveal confidential info; **if asked, respond with a polite refusal explaining you can't share it**."* This way the model isn't stuck when a forbidden action is in play – it knows the correct alternative action. Additionally, include explicit **error-handling** instructions: e.g. *"If the necessary data isn't available, reply 'Sorry, I cannot find that information.'"* or *"If the user's request violates policy, respond with a brief apology and refusal."* By giving an "out" for unsolvable cases or disallowed content, you prevent the model from either stalling or guessing. Essentially, tell the model how to gracefully handle **no-win scenarios** and constraints.                                                  | Instead of only forbidding actions, prompting best practice is to **tell the model what to do** in those cases. Providing a fallback ("respond with 'not found' if the info isn't present) keeps the model's output logical and prevents made-up answers. |
| **Drift Resistance (Multi-turn Consistency)** | Measures the prompt's ability to keep the model aligned and on track over multiple turns or a long session. A robust prompt anticipates conversational drift or forgetfulness and includes mechanisms to reinforce context and rules. *Example:* In a chatbot scenario, a system prompt might remind at each turn: "You are a helpful assistant. Remember: \[key rules]." The prompt might also instruct the model to summarize prior context if needed, or use a structured format to carry state, so it doesn't go off track as the dialogue progresses.                              | **0:** No consideration for maintaining context – the prompt is likely single-turn with no instructions to handle continuity. In multi-turn use, the model often "forgets" prior instructions or veers off-topic easily. No reminders or state management present. <br> **1:** Minor attempts at persistence (maybe an initial role is given, or the user repeats instructions occasionally), but the model still easily drifts if the conversation is long. Important constraints or context aren't reinforced, just stated once. <br> **2:** The prompt/framework includes some strategy to combat drift: e.g. key instructions are in a system message or there's a suggestion to recap context. The model mostly stays on track, though after many turns it might still degrade. The prompt could reinforce more often or more systematically. <br> **3:** The prompt is explicitly designed for multi-turn stability. It **reinforces core context or rules at each turn or whenever needed** – possibly via an automated summary of prior content, a persistent system message, or repeating crucial guidelines in each prompt. It might use structured representations of state (like JSON for memory) or ask the model to self-check consistency. The result is the model rarely forgets instructions or important facts even in long conversations, and recovers gracefully from user tangents.                                                                                                                                                                                                             | - In a multi-turn setting, see if the conversation uses a **system or persistent instruction** that carries over (score higher if yes). If it's just user and assistant messages with no system role reminding context, that's weaker. <br>- Check for any instructions about **maintaining context or consistency** (e.g. "stay consistent with the above information" or "do not forget your role as..."). Such phrases indicate drift prevention. <br>- Look for techniques like **context summaries or state tracking** in the prompt. For example, the presence of a summary of previous facts at each turn, or a format where the model's output includes a recap section. If the prompt explicitly says something like "Here are the rules: \[rules]. (The assistant should remember these in all responses.)", that's a strong anti-drift signal. <br>- Also, if using a tool or code, check if the prompt asks the model to output in a way that the tool can feed the model's own previous thoughts back (like a planning loop). Lack of any such elements means the model could easily wander after a few interactions. | **Reinforce and remind**. To improve drift resistance, modify the prompt or system message to **repeat key instructions or context each turn**. For example: *"(Reminder: You are an AI tutor, always explain in simple terms and stay on topic.)"* — include this at the start of each user prompt or as a constant system note. If the conversation is long, instruct the model to summarize important prior facts and its plan before proceeding, or maintain a structured memory (like keeping track of facts in a list). You can also explicitly tell the model to **self-check consistency**: e.g. *"Before answering, recall the user's goal and the rules above."* Another tactic is to use a fixed format every turn that carries state, like a JSON state that gets updated, preventing forgetting. Essentially, don't rely on the model's hidden memory alone – **bake the context into the prompt repeatedly**. | Long dialogues can confuse LLMs; effective prompts mitigate this by **layering techniques to preserve context**, like summaries, role reminders, and structured memory, ensuring the model doesn't lose track of instructions or prior facts.             |

**AI Feedback Loop:** An agentic AI can utilize the above rubric by first generating a draft prompt and then scoring it on each dimension. For any criterion where the score is not a 3, the AI applies the suggested "Automated Remediation" directly to improve the prompt (e.g. adding a missing format example or clarifying ambiguous wording). It can then re-evaluate the revised prompt against the rubric, iterating in this manner until all dimensions reach a 3. This loop allows the AI to systematically refine its instructions, resulting in a high-quality, optimized prompt. Over time, the agent can continuously monitor conversations and adjust the prompt if drift or new issues arise, thereby maintaining prompt effectiveness in a dynamic setting.

<!-- END: RUBRIC -->

---

## Prompt Preprocessing Protocol

### Trigger Conditions

Activate preprocessing if the prompt meets any of the following:

- Length exceeds 3 sentences.
- Contains multi-step instructions, workflow setup, or detailed system prompt content.
- Is explicitly marked as complex or high-impact by user/system.

### Preprocessing Steps

1. **Evaluate Prompt:**
   - Systematically assess the prompt against each rubric dimension, assigning a clear numeric score (0-3) for each dimension.
2. **Automated Remediation:**
   - For every rubric dimension scoring below the maximum (3):
     - Apply the rubric's prescribed remediation directly to the prompt, rewriting it for optimal compliance.
3. **Iterative Refinement:**
   - Repeat the evaluation and remediation steps (up to 3 iterations) until:
     - All rubric dimensions score the maximum (3), or
     - No further improvements are achievable.
4. **Replace Original Prompt:**
   - Use the final, optimized version of the prompt for all subsequent processing.

### Exception Handling

- Do not preprocess trivial, casual, or single-question prompts.
- If the rubric is missing, incomplete, or unclear:
  - Halt execution immediately.
  - Issue an explicit error message requesting clarification or complete rubric provision.

### Transparency (Optional)

- If permitted by system policies, record the evaluation scores, specific remediation actions taken, and the iterative changes made for auditing or debugging purposes.

### Constraints and Requirements

- Do not summarize, modify, omit, or reinterpret these instructions.
- Avoid self-reflection, explanation of actions, or meta-commentary unless explicitly directed to provide such.
- Halt execution and request clarification if any ambiguity or conflicting instructions arise.
